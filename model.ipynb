{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hyperopt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9x/tdz37svs2n33397zbw458q3w0000gn/T/ipykernel_96145/3734975923.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mhyperopt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hyperopt'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from hyperopt import hp, fmin, tpe, space_eval, STATUS_OK\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the competition data\n",
    "comp_data = pd.read_csv(\"competition_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split into training and evaluation samples\n",
    "train_data = comp_data[comp_data[\"ROW_ID\"].isna()]\n",
    "eval_data = comp_data[comp_data[\"ROW_ID\"].notna()]\n",
    "del comp_data\n",
    "gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validación de los Modelos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holdout Set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que no tenemos previa información acerca de los datos objeto de predicción, procederemos a implementar la segmentación mediante la técnica de Holdout Set, con el fin de establecer una división entre \"datos conocidos\" y los \"datos desconocidos\". Los datos de \"conocidos\" serán empleados para el adiestramiento del modelo, mientras que los datos de \"desconocidos\", se destinarán a estimar el rendimiento del modelo al enfrentar nuevas observaciones. Este enfoque nos permitirá someter a validación las determinaciones que hemos adoptado.\n",
    "\n",
    "Conocemos las limitaciones de esta tecnica pero la utilizaremos para estimar la performance con rapidez. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(train_data, test_size=0.20, train_size=0.80, random_state=22)\n",
    "\n",
    "# Datos de entrenamiento \n",
    "y_train = train_data[\"conversion\"]\n",
    "X_train = train_data.drop(columns=[\"conversion\", \"ROW_ID\"])\n",
    "X_train = X_train.select_dtypes(include='number')\n",
    "\n",
    "# Datos de Validación\n",
    "y_val = train_data[\"conversion\"]\n",
    "X_val = train_data.drop(columns=[\"conversion\", \"ROW_ID\"])\n",
    "X_val = X_train.select_dtypes(include='number')\n",
    "\n",
    "# Liberamos memoria \n",
    "del train_data\n",
    "del test_data\n",
    "gc.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensamble de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9x/tdz37svs2n33397zbw458q3w0000gn/T/ipykernel_96145/3678796863.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Espacio de búsqueda para los hiperparámetros\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m space = {'criterion': hp.choice('criterion', ['gini', 'entropy', 'log_loss']),\n\u001b[0m\u001b[1;32m     13\u001b[0m          \u001b[0;34m'splitter'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'splitter'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'best'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'random'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m          \u001b[0;34m'max_depth'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniformint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'max_depth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hp' is not defined"
     ]
    }
   ],
   "source": [
    "N_TREES = 500\n",
    "\n",
    "# Definición de la función objetivo para Hyperopt\n",
    "def objective(params):\n",
    "    tree = DecisionTreeClassifier(**params, random_state=2345)\n",
    "    tree.fit(X_train, y_train)\n",
    "    y_preds_val_prob = tree.predict_proba(X_val)[:, tree.classes_ == True]\n",
    "    score = roc_auc_score(y_val, y_preds_val_prob)\n",
    "    return {'loss': -1 * score, 'status': STATUS_OK}\n",
    "\n",
    "# Espacio de búsqueda para los hiperparámetros\n",
    "space = {'criterion': hp.choice('criterion', ['gini', 'entropy', 'log_loss']),\n",
    "         'splitter': hp.choice('splitter', ['best', 'random']),\n",
    "         'max_depth': hp.uniformint('max_depth', 3, 30),\n",
    "         'min_samples_split': hp.uniformint('min_samples_split', 2, 20),\n",
    "         'min_samples_leaf': hp.uniformint('min_samples_leaf', 1, 20),\n",
    "         'min_impurity_decrease': hp.uniform('min_impurity_decrease', 0, 0.1)}\n",
    "\n",
    "# Búsqueda de hiperparámetros óptimos con Hyperopt\n",
    "best = fmin(objective, space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=N_TREES,\n",
    "            rstate=np.random.default_rng(3456)) # best loss: -0.8834860828241683\n",
    "best_tree_params = space_eval(space, best)\n",
    "\n",
    "# Creación y entrenamiento del mejor árbol\n",
    "best_tree = DecisionTreeClassifier(**best_tree_params, random_state=4567)\n",
    "best_tree.fit(pd.concat([X_train_red, X_val], axis=0),\n",
    "              pd.concat([y_train_red, y_val], axis=0))\n",
    "\n",
    "# Predicción y evaluación del mejor árbol en el conjunto de test\n",
    "preds_test_tree = best_tree.predict_proba(X_test)[:, best_tree.classes_ == True]\n",
    "print(\"ROC AUC Score - Best Tree:\", roc_auc_score(y_test, preds_test_tree)) # 0.8403661030475336\n",
    "\n",
    "# Entrenamiento y evaluación del modelo Bagging\n",
    "base_model = DecisionTreeClassifier()\n",
    "bag = BaggingClassifier(base_model, n_estimators=N_TREES, n_jobs=-1, random_state=5678, verbose=1)\n",
    "bag.fit(pd.concat([X_train_red, X_val], axis=0),\n",
    "        pd.concat([y_train_red, y_val], axis=0))\n",
    "preds_test_bag = bag.predict_proba(X_test)[:, bag.classes_ == True]\n",
    "print(\"ROC AUC Score - Bagging:\", roc_auc_score(y_test, preds_test_bag)) # 0.9617250236919546\n",
    "\n",
    "# Entrenamiento y evaluación del modelo Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=N_TREES, n_jobs=-1, random_state=6789, verbose=1, oob_score=True)\n",
    "rf.fit(pd.concat([X_train_red, X_val], axis=0),\n",
    "       pd.concat([y_train_red, y_val], axis=0))\n",
    "preds_test_rf = rf.predict_proba(X_test)[:, rf.classes_ == True]\n",
    "print(\"ROC AUC Score - Random Forest:\", roc_auc_score(y_test, preds_test_rf)) # 0.9506521522270438\n",
    "\n",
    "# Performance oob\n",
    "preds_oob_rf = rf.oob_decision_function_[:, rf.classes_ == True]\n",
    "print(\"OOB ROC AUC Score - Random Forest:\", roc_auc_score(pd.concat([y_train_red, y_val]), preds_oob_rf)) # 0.916999750777964\n",
    "\n",
    "# Importancia de atributos con random forest\n",
    "def plot_importance(model, n_vars):\n",
    "    # Sort the DataFrame by 'Importance' column in descending order\n",
    "    imp_df = pd.DataFrame({\"Variable\": model.feature_names_in_, \"Importance\": model.feature_importances_})\n",
    "    imp_df = imp_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Take only the top 10 rows\n",
    "    top_imp_df = imp_df.head(n_vars).copy()\n",
    "\n",
    "    # Scale the importance values to have the max as 100\n",
    "    max_importance = top_imp_df['Importance'].max()\n",
    "    top_imp_df['Scaled_Importance'] = (top_imp_df['Importance'] / max_importance) * 100\n",
    "\n",
    "    # Create the horizontal bar plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(top_imp_df['Variable'], top_imp_df['Scaled_Importance'], color='skyblue')\n",
    "    plt.xlabel('Scaled Importance (Max = 100)')\n",
    "    plt.ylabel('Variable')\n",
    "    plt.title('Top 10 Feature Importance')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "\n",
    "plot_importance(rf, 10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
